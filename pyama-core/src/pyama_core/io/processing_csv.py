"""
Processing CSV format definitions for PyAMA trace data.

This module defines the data structures and utilities for handling CSV files
generated by the processing module. The format includes FOV-level trace data
with cell tracking information.

Format: fov, cell, time, good, position_x, position_y, [dynamic feature columns]
"""

import pandas as pd
from pathlib import Path
from typing import TypedDict
from pyama_core.processing.extraction.trace import ResultIndex, Result


class ProcessingCSVRow(TypedDict, total=False):
    """Row structure for processing CSV files with dynamic feature columns.

    Fields correspond to ResultIndex + Result dataclasses from trace.py plus fov column added during CSV generation.
    """

    # Dynamic feature columns (e.g., intensity_total, area) added at runtime
    __annotations__ = {
        "fov": int,  # Added during CSV export in extraction service
        **ResultIndex.__annotations__,  # cell, time
        **Result.__annotations__,  # good, position_x, position_y
    }


def load_processing_csv(csv_path: Path) -> pd.DataFrame:
    """
    Load trace data from a processing CSV file.

    Args:
        csv_path: Path to the processing CSV file

    Returns:
        DataFrame with trace data

    Raises:
        FileNotFoundError: If the CSV file doesn't exist
        ValueError: If the CSV format is invalid
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV file not found: {csv_path}")

    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        raise ValueError(f"Failed to read CSV file: {e}")

    if df.empty:
        raise ValueError(f"CSV file is empty: {csv_path}")

    # Ensure 'good' column exists with default True values
    if "good" not in df.columns:
        df["good"] = True

    return df


def validate_processing_csv(df: pd.DataFrame) -> bool:
    """
    Validate that the DataFrame has the expected processing CSV format.

    Args:
        df: DataFrame to validate

    Returns:
        True if format is valid, False otherwise
    """
    required_cols = list(ProcessingCSVRow.__annotations__.keys())
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        return False

    # Check data types
    for col in required_cols:
        if col in df.columns:
            try:
                pd.to_numeric(df[col], errors="raise")
            except (ValueError, TypeError):
                return False

    # Check integer columns
    for col in ["fov", "cell"]:
        if col in df.columns:
            numeric_series = pd.to_numeric(df[col], errors="coerce")
            if (
                numeric_series.isnull().any()
                or not (numeric_series == numeric_series.astype(int)).all()
            ):
                return False

    return True


def get_fov_metadata(csv_path: Path) -> dict:
    """
    Extract metadata from a processing CSV file.

    Args:
        csv_path: Path to the processing CSV file

    Returns:
        Dictionary with metadata (fov_index, cell_count, time_count)
    """
    df = load_processing_csv(csv_path)

    metadata = {"file_path": csv_path}

    if "fov" in df.columns and len(df) > 0:
        fov_values = df["fov"].dropna().unique()
        metadata["fov_index"] = int(fov_values[0]) if len(fov_values) == 1 else 0

    if "cell" in df.columns:
        metadata["cell_count"] = df["cell"].nunique()

    if "time" in df.columns:
        metadata["time_count"] = df["time"].nunique()

    return metadata


def filter_good_traces(df: pd.DataFrame) -> pd.DataFrame:
    """
    Filter DataFrame to include only traces marked as 'good'.

    Args:
        df: DataFrame with trace data

    Returns:
        Filtered DataFrame containing only good traces
    """
    if "good" not in df.columns:
        return df

    return df[df["good"]].copy()


def parse_trace_data(csv_path: Path) -> dict:
    """
    Parse trace data from a processing CSV file into a simple dictionary format.

    Args:
        csv_path: Path to the processing CSV file

    Returns:
        Dictionary with trace data organized by cell ID
    """
    df = load_processing_csv(csv_path)

    result = {"cell_ids": [], "features": {}, "positions": {}, "good_cells": set()}

    if "cell" not in df.columns:
        return result

    # Get unique cell IDs
    cell_ids = df["cell"].unique()
    result["cell_ids"] = sorted([int(cid) for cid in cell_ids])

    # Parse good status
    if "good" in df.columns:
        good_cells = df[df["good"]]["cell"].unique()
        result["good_cells"] = set([int(cid) for cid in good_cells])

    # Parse positions if available
    if "position_x" in df.columns and "position_y" in df.columns:
        for cell_id in cell_ids:
            cell_df = df[df["cell"] == cell_id]
            if not cell_df.empty:
                positions = {}
                for _, row in cell_df.iterrows():
                    try:
                        time_point = int(row["time"])
                        px = float(row["position_x"])
                        py = float(row["position_y"])
                        positions[time_point] = (px, py)
                    except (ValueError, TypeError):
                        continue
                if positions:
                    result["positions"][int(cell_id)] = positions

    # Parse feature columns (all columns except the basic ones)
    basic_cols = set(ProcessingCSVRow.__annotations__.keys())
    feature_cols = [col for col in df.columns if col not in basic_cols]

    for feature in feature_cols:
        result["features"][feature] = {}
        for cell_id in cell_ids:
            cell_df = df[df["cell"] == cell_id].sort_values("time")
            if not cell_df.empty:
                values = []
                for _, row in cell_df.iterrows():
                    try:
                        val = float(row[feature])
                        values.append(val)
                    except (ValueError, TypeError):
                        values.append(float("nan"))
                result["features"][feature][int(cell_id)] = values

    return result


def get_cell_count(csv_path: Path) -> int:
    """
    Get the number of unique cells in a processing CSV file.

    Args:
        csv_path: Path to the processing CSV file

    Returns:
        Number of unique cells
    """
    try:
        df = load_processing_csv(csv_path)
        return df["cell"].nunique()
    except Exception:
        return 0
